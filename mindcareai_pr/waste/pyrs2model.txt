import pandas as pd
import numpy as np
import json
import joblib

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.utils.class_weight import compute_class_weight

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import difflib

# Download NLTK resources
nltk.download("stopwords")
nltk.download("wordnet")

# =========================
# Load Dataset
# =========================
df = pd.read_csv("C:/Users/Dhruv/Desktop/mindcareai_project/ml_model/data/isear.csv")

# normalize text and labels
df["Content"] = df["Content"].astype(str)
df["Emotion"] = df["Emotion"].astype(str).str.lower().str.strip()

# fix simple typos in emotion labels using difflib against expected labels
_known = ["joy", "sadness", "anger", "fear", "shame", "disgust", "guilt"]

def _fix(lbl: str) -> str:
    if not isinstance(lbl, str) or lbl == "":
        return lbl
    m = difflib.get_close_matches(lbl, _known, n=1, cutoff=0.6)
    return m[0] if m else lbl

df["Emotion"] = df["Emotion"].apply(_fix)

# drop rows with missing text or labels
df = df[df["Content"].notna() & df["Emotion"].notna() & (df["Emotion"] != "")].copy()

# show counts and drop very low-frequency labels (singletons) that break stratified split
# Iteratively remove any labels that have fewer than 2 examples.
while True:
    label_counts = df["Emotion"].value_counts()
    print("Label counts:\n", label_counts)
    low_labels = label_counts[label_counts < 2].index.tolist()
    if not low_labels:
        break
    print(f"Dropping labels with fewer than 2 examples: {low_labels}")
    df = df[~df["Emotion"].isin(low_labels)].copy()
    if df.empty:
        raise ValueError("No data available after dropping low-frequency labels. Provide a different dataset or remove filtering.")

texts = df["Content"].astype(str)
labels = df["Emotion"].astype(str)


# =========================
# Preprocessing Function
# =========================
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)  # remove URLs
    text = re.sub(r"[^a-z\s]", "", text)        # remove punctuation/numbers
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
    return " ".join(tokens)

texts_cleaned = texts.apply(clean_text)

# =========================
# Train/Test Split
# =========================
# determine if stratified split is possible
counts = labels.value_counts()
if any(counts < 2):
    print("Warning: Some classes have fewer than 2 examples after filtering. Falling back to non-stratified split.")
    strat_param = None
else:
    strat_param = labels

X_train, X_test, y_train, y_test = train_test_split(
    texts_cleaned, labels, test_size=0.2, stratify=strat_param, random_state=42
)

# =========================
# TF-IDF + Logistic Regression Pipeline
# =========================
pipeline = Pipeline([
    ("tfidf", TfidfVectorizer(ngram_range=(1, 2), max_features=30000, min_df=3)),
    ("clf", LogisticRegression(max_iter=200, class_weight="balanced"))
])

# Train
pipeline.fit(X_train, y_train)

# Predict
y_pred = pipeline.predict(X_test)

# =========================
# Evaluation
# =========================
print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

print("\n=== Confusion Matrix ===")
print(confusion_matrix(y_test, y_pred))

acc = accuracy_score(y_test, y_pred)
print(f"\nFinal Accuracy: {acc:.4f}")

# =========================
# Save Model & Metadata
# =========================
joblib.dump(pipeline, "ml_model/emotion_model.pkl")

label_map = {
    "polarity_map": {
        "joy": "positive",
        "fear": "negative",
        "anger": "negative",
        "sadness": "negative",
        "disgust": "negative",
        "shame": "negative",
        "guilt": "negative"
    },
    "risk_map": {
        "joy": "low",
        "fear": "medium",
        "anger": "medium",
        "sadness": "high",
        "disgust": "medium",
        "shame": "high",
        "guilt": "high"
    }
}

with open("ml_model/label_map.json", "w", encoding="utf-8") as f:
    json.dump(label_map, f, indent=4)

print("\nâœ… Model and label_map.json saved successfully!")
